#  LLM-from-Scratch

A simple and extensible transformer-based Language Model built entirely from scratch in PyTorch, trained on the Tiny Shakespeare dataset. This project is part of my ongoing exploration into the inner workings of LLMs, with a strong focus on optimization.

---

##  Project Overview

This repository is a minimal yet complete implementation of a transformer model, covering all the core components of modern LLMs:

- Tokenization (character-level)
- Positional Encoding
- Multi-head Self-Attention
- Transformer Blocks
- Causal Masking
- Training loop with CrossEntropy loss
- Sampling & interactive generation

The goal is not just to train a small LLM, but to understand *how* each building block contributes to the modelâ€™s performance, generalization, and learning stability â€” essential for trustworthy and secure model design.

---

## Academic Context

This project ties directly into my academic focus: building secure, explainable, and efficient AI models. It complements my broader research interests in:

- Model robustness and interpretability
- Secure machine learning and data privacy
- Low-level system understanding of ML architectures
- Exploring how LLMs can be optimized for constrained environments
- 
---

## ğŸ“‚ Directory Structure
llm-from-scratch/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ tiny_text.txt # Shakespeare dataset
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ transformer.py # Core TransformerModel
â”‚ â”œâ”€â”€ attention.py # MultiHeadSelfAttention module
â”‚ â””â”€â”€ embedding.py # embed the token into a vector
â”‚
â”œâ”€â”€ utils.py # Tokenizer and batch functions
â”œâ”€â”€ config.py # Training config
â”œâ”€â”€ train.py # Main training script
â”œâ”€â”€ generate.py # Interactive generation script
â””â”€â”€ README.md

---

## ğŸ› ï¸ Setup & Training

### Requirements

- Python 3.8+
- PyTorch
- tqdm (optional for progress bars)

### Install dependencies

```bash
pip install torch

## Contributions
Pull requests are welcome! Feel free to fork this repo and experiment with:

- Byte-pair tokenization

- Rotary or relative position embeddings

- Training on new datasets

- Adding evaluation metrics (perplexity, BLEU, etc.)

- Performance tuning for different hardware

ğŸ“Œ License
This project is open-sourced under the MIT license.

## ğŸ‘¨â€ğŸ“ About Me
Iâ€™m a Computer Engineering student and researcher focused on AI security, LLM interpretability, and hardware-software co-design for intelligent systems. This project is part of my independent study into building trustable and efficient AI from the ground up.
Feel free to connect on LinkedIn or follow my technical blogs on Hashnode where I write about LLMs, security, and embedded AI.
